{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPECIAL HW - ONLINE STUDENTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 9: L11 - p2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code also checks if the matrix is a square matrix and if it is invertible (i.e., its determinant is not equal to zero). If these conditions are not met, it will inform you that the inverse cannot be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse of A matrix:\n",
      "[[ 1.11022302e-16  1.00000000e+00  0.00000000e+00 -1.11022302e-16]\n",
      " [-5.00000000e-01  5.00000000e-01  0.00000000e+00  5.00000000e-01]\n",
      " [ 1.50000000e+00 -1.50000000e+00  0.00000000e+00 -5.00000000e-01]\n",
      " [-1.50000000e+00  1.50000000e+00  1.00000000e+00  5.00000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A_matrix = [\n",
    "    [1, 1, 1, 0],\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 1, 1],\n",
    "    [0, 3, 1, 0],\n",
    "]\n",
    "\n",
    "def compute_inverse(matrix):\n",
    "    # Check if the matrix is square\n",
    "    rows = len(matrix)\n",
    "    cols = len(matrix[0])\n",
    "\n",
    "    if rows != cols:\n",
    "        print(\"The matrix is not square, so the inverse cannot be computed.\")\n",
    "        return None\n",
    "\n",
    "    # Convert the matrix to a NumPy array\n",
    "    matrix_np = np.array(matrix)\n",
    "\n",
    "    # Check if the matrix is invertible\n",
    "    det = np.linalg.det(matrix_np)\n",
    "    if det == 0:\n",
    "        print(\"The matrix is not invertible, so the inverse cannot be computed.\")\n",
    "        return None\n",
    "\n",
    "    # Compute the inverse\n",
    "    inverse = np.linalg.inv(matrix_np)\n",
    "    return inverse\n",
    "\n",
    "A_inverse = compute_inverse(A_matrix)\n",
    "if A_inverse is not None:\n",
    "    print(\"Inverse of A matrix:\")\n",
    "    print(A_inverse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A_inervse_matrix = [  \n",
    "    [0, 1, 0, 0],  \n",
    "    [-0.5, 0.5, 0, 0.5],  \n",
    "    [1.5, -1.5, 0, -0.5],  \n",
    "    [-1.5, 1.5, 1, 0.5],  \n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW10: L13 - p3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the next optimization problem: \n",
    "\n",
    "min x1 + x2 + x3 + x4 + x5\n",
    "\n",
    "A matrix = \n",
    "3, 2, 1, 0, 0\n",
    "5, 1, 1, 1, 0\n",
    "2, 5, 1, 0, 1\n",
    "\n",
    "b_ matrix = \n",
    "1\n",
    "3\n",
    "4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Hungarian method with the SciPy library as for the project: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min (x1 + x2 + x3 + x4 + x5)   \n",
    "A * x >= b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "\n",
    "# Objective function coefficients\n",
    "c = [1, 1, 1, 1, 1]\n",
    "\n",
    "# Inequality constraint matrix and vector\n",
    "A = [\n",
    "    [-3, -2, -1,  0,  0],\n",
    "    [-5, -1, -1, -1,  0],\n",
    "    [-2, -5, -1,  0, -1],\n",
    "]\n",
    "b = [-1, -3, -4]\n",
    "\n",
    "# Boundaries for variables (x1, x2, x3, x4, x5) - assuming non-negative variables\n",
    "bounds = [(0, None) for _ in range(len(c))]\n",
    "\n",
    "# Solving the linear programming problem\n",
    "result = linprog(c, A_ub=A, b_ub=b, bounds=bounds, method=\"highs\")\n",
    "\n",
    "print(\"Optimal solution:\")\n",
    "print(result.x)\n",
    "print(\"Minimum value of the objective function:\")\n",
    "print(result.fun)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max (x1 + x2 + x3 + x4 + x5)   \n",
    "A * x <= b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution:\n",
      "[0. 0. 0. 3. 4.]\n",
      "Maximum value of the objective function:\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "\n",
    "# Objective function coefficients (negated for maximization)\n",
    "c = [-1, -1, -1, -1, -1]\n",
    "\n",
    "# Inequality constraint matrix and vector\n",
    "A = [\n",
    "    [3, 2, 1, 0, 0],\n",
    "    [5, 1, 1, 1, 0],\n",
    "    [2, 5, 1, 0, 1],\n",
    "]\n",
    "b = [1, 3, 4]\n",
    "\n",
    "# Boundaries for variables (x1, x2, x3, x4, x5) - assuming non-negative variables\n",
    "bounds = [(0, None) for _ in range(len(c))]\n",
    "\n",
    "# Solving the linear programming problem\n",
    "result = linprog(c, A_ub=A, b_ub=b, bounds=bounds, method=\"highs\")\n",
    "\n",
    "print(\"Optimal solution:\")\n",
    "print(result.x)\n",
    "print(\"Maximum value of the objective function:\")\n",
    "print(-result.fun)  # Negate the result to obtain the maximum value\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW11: L14 - p2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the current basis matrix B and cost vector c, let's find the basic feasible solution, calculate the reduced costs, and update the basis using the simplex algorithm.\n",
    "\n",
    "The current basis matrix B and the basic variables are:\n",
    "\n",
    "$$\n",
    "B = \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix} \\quad \\text{with basic variables: } x_1, x_4, x_5\n",
    "$$\n",
    "\n",
    "To find the basic feasible solution, solve B * x_B = b:\n",
    "\n",
    "$$\n",
    "x_B = B^{-1} * b = \\begin{bmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} * \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_B = \\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, the basic feasible solution is x1 = 1, x4 = 2, x5 = 4, and the non-basic variables are x2 = x3 = 0.\n",
    "\n",
    "Now let's find the reduced costs by calculating c - c_B * A:\n",
    "\n",
    "$$\n",
    "c_B = [1, 1, 1] \\\\\n",
    "A = \\begin{bmatrix} 3 & 2 & 1 & 0 & 0 \\\\ 5 & 1 & 1 & 1 & 0 \\\\ 2 & 5 & 1 & 0 & 1 \\end{bmatrix} \\\\\n",
    "c - c_B * A = [1, 1, 1, 1, 1] - [1, 1, 1] * A\n",
    "$$\n",
    "\n",
    "$$\n",
    "c - c_B * A = [0, -2, 0, 0, 0]\n",
    "$$\n",
    "\n",
    "The reduced cost for x2 is -2, which is negative, indicating that we can improve the objective function by introducing x2 into the basis. To find the leaving variable, we calculate the minimum ratio test:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{RHS}}{\\text{column of entering variable }(x_2)} = \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{3}{1} \\\\ \\frac{4}{5} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Minimum ratio = 1/2\n",
    "\n",
    "Since 1/2 is the smallest ratio, x1 will leave the basis, and x2 will enter. The new basis matrix B' and cost vector c' are:\n",
    "\n",
    "$$\n",
    "B' = \\begin{bmatrix} 2 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 5 & 0 & 1 \\end{bmatrix} \\quad \\text{with basic variables: } x_2, x_4, x_5 \\\\\n",
    "c' = [0, 1, 1, 1, 1]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing the simplex algorithm with the new basis and cost vector:\n",
    "\n",
    "The new basis matrix B' and the basic variables are:\n",
    "\n",
    "$$\n",
    "B' = \\begin{bmatrix} 2 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 5 & 0 & 1 \\end{bmatrix} \\quad \\text{with basic variables: } x_2, x_4, x_5\n",
    "$$\n",
    "\n",
    "To find the new basic feasible solution, solve B' * x_B' = b:\n",
    "\n",
    "$$\n",
    "x_B' = B'^{-1} * b = \\begin{bmatrix} 1/2 & 0 & 0 \\\\ -1/2 & 1 & 0 \\\\ -5/2 & 0 & 1 \\end{bmatrix} * \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_B' = \\begin{bmatrix} 1/2 \\\\ 5/2 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, the new basic feasible solution is x2 = 1/2, x4 = 5/2, x5 = 2, and the non-basic variables are x1 = x3 = 0.\n",
    "\n",
    "Now let's find the reduced costs by calculating c' - c_B' * A:\n",
    "\n",
    "$$\n",
    "c_B' = [0, 1, 1] \\\\\n",
    "A = \\begin{bmatrix} 3 & 2 & 1 & 0 & 0 \\\\ 5 & 1 & 1 & 1 & 0 \\\\ 2 & 5 & 1 & 0 & 1 \\end{bmatrix} \\\\\n",
    "c' - c_B' * A = [1, 0, 1, 1, 1] - [0, 1, 1] * A\n",
    "$$\n",
    "\n",
    "$$\n",
    "c' - c_B' * A = [1, 0, 1, 1, 1]\n",
    "$$\n",
    "\n",
    "There are no negative reduced costs, which means we have found the optimal solution. The optimal solution is x2 = 1/2, x4 = 5/2, x5 = 2, with x1 = x3 = 0, and the minimum value of the objective function is:\n",
    "\n",
    "$$\n",
    "Z_{min} = x_1 + x_2 + x_3 + x_4 + x_5 = 0 + \\frac{1}{2} + 0 + \\frac{5}{2} + 2 = 5\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW12: L17&18 - 94"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how to find a flow from s to t of value f with minimum cut and transform it into a transshipment problem, I'll use an example of a flow network. Let's consider the following flow network:\n",
    "\n",
    "Nodes: s, a, b, c, t\n",
    "Edges with capacities:\n",
    "\n",
    "s -> a: 4  \n",
    "s -> b: 3  \n",
    "a -> c: 3  \n",
    "b -> c: 2  \n",
    "b -> a: 1  \n",
    "c -> t: 5  \n",
    "\n",
    "Find a flow from s to t of value f with minimum cut:  \n",
    "\n",
    "Using the Ford-Fulkerson algorithm or the Edmonds-Karp algorithm, we can find the maximum flow and the corresponding minimum cut.  \n",
    "\n",
    "In this example, the maximum flow is 5, and the flow along the edges is as follows:  \n",
    "\n",
    "s -> a: 3  \n",
    "s -> b: 2  \n",
    "a -> c: 3   \n",
    "b -> c: 2  \n",
    "b -> a: 0  \n",
    "c -> t: 5  \n",
    "\n",
    "Transform the flow problem into a transshipment problem:  \n",
    "\n",
    "To transform the flow problem into a transshipment problem, we introduce a new commodity k, with a demand of 5 units from s to t. We then modify the flow network to include the demand and supply constraints at each node. The transshipment problem has the following properties:  \n",
    "\n",
    "Nodes: s, a, b, c, t  \n",
    "Edges with capacities remain the same.  \n",
    "Demands (supply or consumption) at each node:  \n",
    "\n",
    "s: -5 (supply)  \n",
    "a: 0  \n",
    "b: 0  \n",
    "c: 0  \n",
    "t: 5 (demand)  \n",
    "\n",
    "**Prove that flow conservation implies demand satisfaction:**   \n",
    "\n",
    "Flow conservation means that for each node in the network (except for the source and sink), the sum of incoming flows equals the sum of outgoing flows. We have already shown that the maximum flow in the network is 5 units, which equals the demand from s to t. Since the flow conservation is satisfied for all nodes, the flow of 5 units must reach the sink node t, implying that the demand is satisfied.  \n",
    "\n",
    "**Prove that demand satisfaction implies flow conservation:**    \n",
    "\n",
    "If the demand at the sink node t is satisfied, this means that a flow of 5 units has reached node t. The flow must pass through the intermediate nodes (a, b, c) while respecting the capacities of the edges. Since the capacities are not exceeded and the demand is satisfied, this implies that the flow conservation is maintained at each node in the network.  \n",
    "\n",
    "In conclusion, we have found a flow of value 5 with a minimum cut and transformed the problem into a transshipment problem. We have shown that flow conservation leads to demand satisfaction and vice versa.  \n",
    "\n",
    "As further clarification for the proof that the demand satisfaction implies flow conservation, some mathematical developments will be shown.\n",
    "\n",
    "Let G = (V, E) be a directed graph representing a transshipment problem, where V is the set of vertices (nodes) and E is the set of edges (arcs). Let x_ij denote the flow on the arc (i, j) ∈ E, and let d_i denote the demand at node i. A positive demand indicates supply, while a negative demand indicates consumption. \n",
    "\n",
    "To prove that demand satisfaction implies flow conservation, we must show that for each node i ∈ V (except for the source and sink), the sum of incoming flows equals the sum of outgoing flows, given that the demand is satisfied.\n",
    "\n",
    "**Lemma:** The flow x satisfies the demand d at each node i if and only if the following equation holds:\n",
    "\n",
    "$$\n",
    "\\sum_{j:(i,j) \\in E} x_{ij} - \\sum_{j:(j,i) \\in E} x_{ji} = d_i \\quad \\forall i \\in V\n",
    "$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "(⇒) Suppose that the flow x satisfies the demand d at each node i. By definition, this means that the net flow into node i equals the demand at that node. Thus, the sum of incoming flows minus the sum of outgoing flows equals the demand:\n",
    "\n",
    "$$\n",
    "\\sum_{j:(i,j) \\in E} x_{ij} - \\sum_{j:(j,i) \\in E} x_{ji} = d_i \\quad \\forall i \\in V\n",
    "$$\n",
    "\n",
    "(⇐) Now, suppose the equation holds:\n",
    "\n",
    "$$\n",
    "\\sum_{j:(i,j) \\in E} x_{ij} - \\sum_{j:(j,i) \\in E} x_{ji} = d_i \\quad \\forall i \\in V\n",
    "$$\n",
    "\n",
    "This implies that for each node i, the net flow into the node (the sum of incoming flows minus the sum of outgoing flows) equals the demand at that node. Thus, the flow x satisfies the demand d at each node i.\n",
    "\n",
    "Now, let's consider any node i ∈ V (excluding the source and sink). Since the demand is satisfied, we have:\n",
    "\n",
    "$$\n",
    "\\sum_{j:(i,j) \\in E} x_{ij} - \\sum_{j:(j,i) \\in E} x_{ji} = d_i = 0\n",
    "$$\n",
    "\n",
    "Rearranging the terms, we get:\n",
    "\n",
    "$$\n",
    "\\sum_{j:(i,j) \\in E} x_{ij} = \\sum_{j:(j,i) \\in E} x_{ji}\n",
    "$$\n",
    "\n",
    "This equation states that for each node i (except for the source and sink), the sum of incoming flows equals the sum of outgoing flows, which is the definition of flow conservation.\n",
    "\n",
    "Thus, we have shown that demand satisfaction implies flow conservation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW13: L21 - Verify the dual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primal problem:**\n",
    "\n",
    "Minimize:\n",
    "\n",
    "$$\n",
    "W = \\sum_{i,j} c_{ij} \\cdot x_{ij}\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{j} x_{ij} \\geq 1 \\quad \\forall i \\quad (\\text{corresponding to } \\alpha_i \\text{ variables in primal}) \\\\\n",
    "\\sum_{i} x_{ij} \\geq 1 \\quad \\forall j \\quad (\\text{corresponding to } \\beta_j \\text{ variables in primal}) \\\\\n",
    "x_{ij} \\geq 0 \\quad \\forall i, j\n",
    "$$\n",
    "\n",
    "\n",
    "**Dual problem:**\n",
    "\n",
    "Maximize:\n",
    "\n",
    "$$\n",
    "Z = \\sum_{i} \\alpha_i + \\sum_{j} \\beta_j\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "\\alpha_i + \\beta_j \\leq c_{ij} \\quad \\forall i, j \\\\\n",
    "\\alpha_i \\geq 0 \\quad \\forall i \\\\\n",
    "\\beta_j \\geq 0 \\quad \\forall j\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the weak duality theorem for the given primal and dual linear programs, we need to show that the objective value of the primal is always greater than or equal to the objective value of the dual.\n",
    "\n",
    "For the given primal and dual problems, consider any feasible solution for both the primal and the dual problems.\n",
    "\n",
    "**Primal objective value:**\n",
    "\n",
    "$$\n",
    "W = \\sum_{i,j} c_{ij} \\cdot x_{ij}\n",
    "$$\n",
    "\n",
    "**Dual objective value:**\n",
    "\n",
    "$$\n",
    "Z = \\sum_{i} \\alpha_i + \\sum_{j} \\beta_j\n",
    "$$\n",
    "\n",
    "Since the primal constraints are:\n",
    "\n",
    "$$\n",
    "\\sum_{j} x_{ij} \\geq 1 \\quad \\forall i \\\\\n",
    "\\sum_{i} x_{ij} \\geq 1 \\quad \\forall j \\\\\n",
    "x_{ij} \\geq 0 \\quad \\forall i, j\n",
    "$$\n",
    "\n",
    "And the dual constraints are:\n",
    "\n",
    "$$\n",
    "\\alpha_i + \\beta_j \\leq c_{ij} \\quad \\forall i, j \\\\\n",
    "\\alpha_i \\geq 0 \\quad \\forall i \\\\\n",
    "\\beta_j \\geq 0 \\quad \\forall j\n",
    "$$\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\n",
    "\\sum_{i} \\alpha_i + \\sum_{j} \\beta_j = Z \\leq \\sum(\\alpha_i + \\beta_j) \\leq \\sum_{i,j} c_{ij} \\cdot x_{ij} = W\n",
    "$$\n",
    "\n",
    "Thus, we have shown that Z <= W, verifying the weak duality theorem for the given primal and dual linear programs.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW14: L22"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 13 and 14 are the same "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case, I will finish the last proof of the L22"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the complexity and correctness proof of the Hungarian method**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hungarian method, also known as the Kuhn-Munkres algorithm or Munkres assignment algorithm, is a combinatorial optimization algorithm that solves the assignment problem in polynomial time. The assignment problem is a special case of the linear sum assignment problem, where the goal is to find the minimum-weight perfect matching in a weighted bipartite graph.\n",
    "\n",
    "**Complexity:**\n",
    "\n",
    "The Hungarian algorithm has a worst-case time complexity of O(n^3), where n is the number of vertices on each side of the bipartite graph. This complexity is achieved by first transforming the original cost matrix into a matrix that has a zero in every row and column (which takes O(n^2) time) and then repeatedly searching for augmenting paths and updating the labels (which takes O(n^3) time).\n",
    "\n",
    "**Correctness proof:**\n",
    "\n",
    "The Hungarian algorithm is based on the concepts of feasible vertex labeling and equality subgraph. The algorithm maintains the invariant that the current labeling is feasible, and the equality subgraph has a perfect matching if and only if the labeling is optimal. The proof of correctness can be broken down into the following steps:\n",
    "\n",
    "- Initialization: The algorithm starts with an initial feasible labeling (e.g., by subtracting the minimum value in each row and then in each column). It is easy to see that the initial labeling is feasible because it satisfies the constraints of the primal problem.\n",
    "\n",
    "- Maintaining feasibility: During the execution of the algorithm, the labels are updated by changing the dual variables in a manner that maintains the feasibility of the labeling. This is done by selecting a set of rows and columns and updating the labels according to the rules of the algorithm.\n",
    "\n",
    "- Optimality: The algorithm terminates when a perfect matching is found in the equality subgraph of the current labeling. At this point, the algorithm has found an optimal solution because the current labeling is both feasible and has a perfect matching in the equality subgraph. By the complementary slackness theorem, an optimal solution to the dual problem can be found by setting x_{ij} = 1 if the corresponding edge is in the perfect matching and x_{ij} = 0 otherwise. Since the primal and dual problems have the same objective value at optimality, the algorithm has found an optimal solution to the assignment problem.\n",
    "\n",
    "- Termination: The algorithm terminates in polynomial time because each iteration either improves the current solution or increases the size of the partial matching. Since there are at most n iterations, the total number of iterations is bounded by O(n^3).\n",
    "\n",
    "In summary, the Hungarian method is a polynomial-time algorithm for solving the assignment problem, and its correctness can be proved using the concepts of feasible labeling, equality subgraph, and complementary slackness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
